# FIXME: define a frontend & backend network, and only expose backend services to the frontend (nginx)

networks:
  backend:

secrets:
  synapse_signing_key:
    file: secrets/synapse/signing.key
  livekit_api_key:
    file: secrets/livekit/livekit_api_key
  livekit_secret_key:
    file: secrets/livekit/livekit_secret_key

services:
  # dependencies for optionally generating default configs + secrets
  generate-synapse-secrets:
    image: ghcr.io/element-hq/synapse:latest
    user: $USER_ID:$GROUP_ID
    restart: "no"
    profiles: ["manual_launch_only"]
    volumes:
      - ${VOLUME_PATH}/data/synapse:/data:rw,z
    env_file: .env
    environment:
      SYNAPSE_CONFIG_DIR: /data
      SYNAPSE_CONFIG_PATH: /data/homeserver.yaml.default
      SYNAPSE_SERVER_NAME: ${DOMAIN}
      SYNAPSE_REPORT_STATS: ${REPORT_STATS}
    # entrypoint: "/entrypoint.sh"

  generate-mas-secrets:
    image: ghcr.io/element-hq/matrix-authentication-service:latest
    user: $USER_ID:$GROUP_ID
    restart: "no"
    profiles: ["manual_launch_only"]
    volumes:
      - ${VOLUME_PATH}/data/mas:/data:rw,z

  # dependency for templating /data-template into /data (having extracted any secrets from any default generated configs)
  init:
    build: init
    user: $USER_ID:$GROUP_ID
    restart: "no"
    profiles: ["manual_launch_only"]
    volumes:
      - ${VOLUME_PATH}/secrets:/secrets:z
      - ${VOLUME_PATH}/data:/data:z
      - ${VOLUME_PATH}/data-template:/data-template:z,ro
      - ${VOLUME_PATH}/init/init.sh:/init.sh:z,ro
    command: "/init.sh"
    env_file: .env
    environment:
      HOMESERVER_FQDN: "matrix.$DOMAIN"
      MAS_FQDN: "auth.$DOMAIN"
      ELEMENT_WEB_FQDN: "webchat.$DOMAIN"
      ELEMENT_CALL_FQDN: "call.$DOMAIN"
      LIVEKIT_FQDN: "livekit.$DOMAIN"
      LIVEKIT_JWT_FQDN: "livekit-jwt.$DOMAIN"
      MAIL_NOTIF_FROM_ADDRESS: "noreply@$DOMAIN"
      ABUSE_SUPPORT_EMAIL: "abuse@$DOMAIN"
      SECURITY_SUPPORT_EMAIL: "security@$DOMAIN"

  caddy:
    image: docker.io/caddy:$CADDY_VERSION
    user: $USER_ID:$GROUP_ID
    restart: always
    env_file: .env
    environment:
      HOMESERVER_FQDN: "matrix.$DOMAIN"
      MAS_FQDN: "auth.$DOMAIN"
      ELEMENT_WEB_FQDN: "webchat.$DOMAIN"
      ELEMENT_CALL_FQDN: "call.$DOMAIN"
      LIVEKIT_FQDN: "livekit.$DOMAIN"
      LIVEKIT_JWT_FQDN: "livekit-jwt.$DOMAIN"
    cap_add:
      - NET_ADMIN
    ports:
      - "8080:8080"
      - "8443:8443"
      - "8443:8443/udp"
      - "8448:8448"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro,z
      - ${VOLUME_PATH}/data/caddy/site:/srv:z
      - ${VOLUME_PATH}/data/caddy/data:/data:z
      - caddy_config:/config
    networks:
      backend:
        aliases: # so our containers can resolve the LB
          - $DOMAIN
          - $HOMESERVER_FQDN
          - $ELEMENT_WEB_FQDN
          - $ELEMENT_CALL_FQDN
          - $MAS_FQDN
    healthcheck:
      # https://stackoverflow.com/a/47722899/5008962, https://github.com/compose-spec/compose-spec/blob/main/spec.md#healthcheck
      test: wget --no-verbose --tries=1 --spider http://127.0.0.1:2019/metrics || exit 1
      interval: 1m30s
      timeout: 10s
      retries: 3
      start_period: 5s

  db-synapse: &db-template
    user: $USER_ID:$GROUP_ID
    image: docker.io/postgres:$POSTGRES_VERSION
    restart: always
    shm_size: 256mb
    networks: []
    healthcheck:
      test: ['CMD', 'pg_isready', '-U', "$POSTGRES_USER", '-d', 'postgres']
      start_period: 5s
      start_interval: 1s
    volumes:
      - ${VOLUME_PATH}/data/postgres/synapse:/var/lib/postgresql/data:z
      - db_sock_synapse:/var/run/postgresql
    environment:
      POSTGRES_INITDB_ARGS: --encoding=UTF8 --locale=C -c listen_addresses='' # disable tcp
      POSTGRES_HOST_AUTH_METHOD: trust
      POSTGRES_USER: $POSTGRES_USER
      POSTGRES_DB: postgres

  db-mas:
    <<: *db-template
    volumes:
      - ${VOLUME_PATH}/data/postgres/mas:/var/lib/postgresql/data:z
      - db_sock_mas:/var/run/postgresql

  redis:
    image: docker.io/redis:latest
    restart: always
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      start_period: 5s
      start_interval: 1s
    networks:
      - backend

  synapse: &synapse-template
    image: ghcr.io/element-hq/synapse:latest
    user: $USER_ID:$GROUP_ID
    restart: always
    volumes:
      - ${VOLUME_PATH}/data/synapse:/data:z
      - ${VOLUME_PATH}/data/caddy/data/caddy/pki/authorities/local/root.crt:/etc/ssl/certs/ca-certificates.crt:ro,z
      - db_sock_synapse:/var/run/postgresql
    # ports:
    #   - 8008:8008
    networks:
      - backend
    environment:
      SYNAPSE_CONFIG_DIR: /data
      SYNAPSE_CONFIG_PATH: /data/homeserver.yaml
    secrets:
      - synapse_signing_key
    depends_on:
      caddy:
        condition: service_healthy
      redis:
        condition: service_started
      db-synapse:
        condition: service_healthy

  synapse-generic-worker-1:
    <<: *synapse-template
    entrypoint: ["/start.py", "run", "--config-path=/data/homeserver.yaml", "--config-path=/data/workers/synapse-generic-worker-1.yaml"]
    healthcheck:
      test: curl -fSs http://localhost:8081/health || exit 1
      # start_period: 5s
      # interval: 15s
      # timeout: 5s
    environment:
      SYNAPSE_WORKER: synapse.app.generic_worker
    # ports:
    #   - 8081:8081
    depends_on:
      - synapse

  synapse-federation-sender-1:
    <<: *synapse-template
    entrypoint: ["/start.py", "run", "--config-path=/data/homeserver.yaml", "--config-path=/data/workers/synapse-federation-sender-1.yaml"]
    healthcheck:
      disable: true
    environment:
      SYNAPSE_WORKER: synapse.app.federation_sender
    # ports:
    #   - 8081:8081
    depends_on:
      - synapse

  mas:
    image: ghcr.io/element-hq/matrix-authentication-service:latest
    user: $USER_ID:$GROUP_ID
    restart: always
    # ports:
    #   - 8083:8080
    volumes:
      - ${VOLUME_PATH}/data/mas:/data:z
      - ${VOLUME_PATH}/data/caddy/data/caddy/pki/authorities/local/root.crt:/etc/ssl/certs/ca-certificates.crt:ro,z
      - db_sock_mas:/var/run/postgresql
    networks:
      - backend
    # FIXME: do we also need to sync the db?
    command: "server --config=/data/config.yaml"
    environment:
      MAS_EMAIL_FROM: "\"Matrix Authentication Service\" <support@${DOMAIN}>"
      MAS_EMAIL_REPLY_TO: "\"Matrix Authentication Service\" <support@${DOMAIN}>"
    depends_on:
      db-mas:
        condition: service_healthy

  # as a basic local MTA
  mailhog:
    image: docker.io/mailhog/mailhog:latest
    restart: always
    ports:
      - 8025:8025
      # - 1025:1025
    networks:
      - backend

  element-web:
    image: docker.io/vectorim/element-web:latest
    restart: always
    # ports:
    #   - 8080:80
    healthcheck:
      test: wget -q -O /dev/null http://localhost:80/version || exit 1
      # start_period: 5s
      # interval: 15s
      # timeout: 5s
    networks:
      - backend
    volumes:
      - ${VOLUME_PATH}/data/element-web/config.json:/app/config.json:z
    depends_on:
      init:
        condition: service_completed_successfully
  #
  # element-call:
  #   image: ghcr.io/element-hq/element-call:latest-ci
  #   restart: unless-stopped
  #   # ports:
  #   #   - 8082:80
  #   networks:
  #     - backend
  #   volumes:
  #     - ${VOLUME_PATH}/data/element-call/config.json:/app/config.json
  #   depends_on:
  #     init:
  #       condition: service_completed_successfully
  #
  # livekit:
  #   image: docker.io/livekit/livekit-server:latest
  #   restart: unless-stopped
  #   volumes:
  #     - ${VOLUME_PATH}/data/livekit/config.yaml:/etc/livekit.yaml
  #   command: --config /etc/livekit.yaml --node-ip ${LIVEKIT_NODE_IP}
  #   ports:
  #     # - 7880:7880 # HTTP listener
  #     - 7881:7881 # TCP WebRTC transport, advertised via SDP
  #
  #     # TODO: expose livekit-turn on TCP & UDP 443 via nginx
  #     # At least this would allow UDP turn on port 443 for better perf.
  #
  #     # You can't expose a massive range here as it literally sets up 10,000 userland listeners, which takes forever
  #     # and will clash with any existing high-numbered ports.
  #     # So for now, tunnel everything via TCP 7881. FIXME!
  #     #- 50000-60000:50000-60000/tcp # TCP media
  #     #- 50000-60000:50000-60000/udp # UDP media
  #   networks:
  #     - backend
  #   depends_on:
  #     init:
  #       condition: service_completed_successfully
  #     redis:
  #       condition: service_started
  #
  # livekit-jwt:
  #   build:
  #     # evil hack to pull in bash so we can run an entrypoint.sh
  #     # FIXME: it's a bit wasteful; the alternative would be to modify lk-jwt-service to pick up secrets from disk
  #     # Another alternative would be to factor out secret generation from compose.yml and create an .env up front
  #     dockerfile_inline: |
  #       FROM ghcr.io/element-hq/lk-jwt-service:latest-ci AS builder
  #       FROM alpine:latest
  #       RUN apk update && apk add bash
  #       COPY --from=builder /lk-jwt-service /
  #   restart: unless-stopped
  #   volumes:
  #     - ${VOLUME_PATH}/data/ssl/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt
  #     - ${VOLUME_PATH}/scripts/livekit-jwt-entrypoint.sh:/entrypoint.sh
  #   entrypoint: /entrypoint.sh
  #   env_file: .env
  #   deploy:
  #     restart_policy:
  #       condition: on-failure
  #   networks:
  #     - backend
  #   secrets:
  #     - livekit_api_key
  #     - livekit_secret_key
  #   depends_on:
  #     init:
  #       condition: service_completed_successfully
  #     livekit:
  #       condition: service_started

volumes:
  caddy_config:
  db_sock_synapse:
  db_sock_mas:
