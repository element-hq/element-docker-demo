# FIXME: define a frontend & backend network, and only expose backend services to the frontend (nginx)

networks:
  backend:

secrets:
  synapse_signing_key:
    file: secrets/synapse/signing.key
  livekit_api_key:
    file: secrets/livekit/livekit_api_key
  livekit_secret_key:
    file: secrets/livekit/livekit_secret_key

services:
  # dependencies for generating configs + secrets used by setup.sh
  generate-synapse-secrets:
    image: ghcr.io/element-hq/synapse:$SYNAPSE_VERSION
    user: $USER_ID:$GROUP_ID
    restart: "no"
    profiles: ["init"]
    volumes:
      - ${VOLUME_PATH}/data/synapse:/data:rw,z
    env_file: .env
    environment:
      SYNAPSE_CONFIG_DIR: /data
      SYNAPSE_CONFIG_PATH: /data/homeserver.yaml
      SYNAPSE_SERVER_NAME: ${DOMAIN}
      SYNAPSE_REPORT_STATS: ${REPORT_STATS}
    # entrypoint: "/entrypoint.sh"

  generate-mas-secrets:
    image: ghcr.io/element-hq/matrix-authentication-service:$MAS_VERSION
    user: $USER_ID:$GROUP_ID
    restart: "no"
    profiles: ["init"]
    env_file: .env
    volumes:
      - ${VOLUME_PATH}/data/mas:/data:rw,z

  init:
    build: init
    user: $USER_ID:$GROUP_ID
    restart: "no"
    profiles: ["init"]
    volumes:
      - ${VOLUME_PATH}/secrets:/secrets:z
      - ${VOLUME_PATH}/data:/data:z
      - ${VOLUME_PATH}/data-template:/data-template:z,ro
      - ${VOLUME_PATH}/init/init.sh:/init.sh:z,ro
    command: "/init.sh"
    env_file: .env

  caddy:
    image: docker.io/caddy:$CADDY_VERSION
    user: $USER_ID:$GROUP_ID
    restart: always
    env_file: .env
    cap_add:
      - NET_ADMIN
    ports:
      - "8080:8080"
      - "8443:8443"
      - "8443:8443/udp"
      - "8448:8448"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro,z
      - ${VOLUME_PATH}/data/caddy/srv:/srv:ro,z
      - ${VOLUME_PATH}/data/caddy/data:/data:z
      - caddy_config:/config
    networks:
      backend:
        aliases: # so our containers can resolve the LB
          - $DOMAIN
          - $HOMESERVER_FQDN
          - $ELEMENT_WEB_FQDN
          - $ELEMENT_CALL_FQDN
          - $MAS_FQDN
    healthcheck:
      # https://stackoverflow.com/a/47722899/5008962, https://github.com/compose-spec/compose-spec/blob/main/spec.md#healthcheck
      test: wget --no-verbose --tries=1 --spider http://127.0.0.1:2019/metrics || exit 1
      interval: 1m30s
      timeout: 10s
      retries: 3
      start_period: 5s

  db-synapse: &db-template
    user: $USER_ID:$GROUP_ID
    image: docker.io/postgres:$POSTGRES_VERSION
    restart: always
    shm_size: 256mb
    networks: []
    healthcheck:
      test: ['CMD', 'pg_isready', '-U', "$POSTGRES_USER", '-d', 'postgres']
      start_period: 5s
      start_interval: 1s
    volumes:
      - ${VOLUME_PATH}/data/postgres/synapse:/var/lib/postgresql/data:z
      - db_sock_synapse:/var/run/postgresql
    environment:
      POSTGRES_INITDB_ARGS: --encoding=UTF8 --locale=C -c listen_addresses='' # disable tcp
      POSTGRES_HOST_AUTH_METHOD: trust
      POSTGRES_USER: $POSTGRES_USER
      POSTGRES_DB: postgres

  db-mas:
    <<: *db-template
    volumes:
      - ${VOLUME_PATH}/data/postgres/mas:/var/lib/postgresql/data:z
      - db_sock_mas:/var/run/postgresql

  redis:
    image: docker.io/redis:$REDIS_VERSION
    restart: always
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      start_period: 5s
      start_interval: 1s
    networks:
      - backend

  synapse: &synapse-template
    image: ghcr.io/element-hq/synapse:$SYNAPSE_VERSION
    user: $USER_ID:$GROUP_ID
    restart: always
    volumes:
      - ${VOLUME_PATH}/data/synapse:/data:z
      - ${VOLUME_PATH}/data/caddy/data/caddy/pki/authorities/local/root.crt:/etc/ssl/certs/ca-certificates.crt:ro,z
      - db_sock_synapse:/var/run/postgresql
    # ports:
    #   - 8008:8008
    networks:
      - backend
    environment:
      SYNAPSE_CONFIG_DIR: /data
      SYNAPSE_CONFIG_PATH: /data/homeserver.yaml
    secrets:
      - synapse_signing_key
    depends_on:
      caddy:
        # caddy needs to create PKI CA (root.crt) first
        condition: service_healthy
      redis:
        condition: service_started
      db-synapse:
        # postgres needs to create socket first
        condition: service_healthy

  synapse-generic-worker-1:
    <<: *synapse-template
    entrypoint: ["/start.py", "run", "--config-path=/data/homeserver.yaml", "--config-path=/data/workers/synapse-generic-worker-1.yaml"]
    healthcheck:
      test: curl -fSs http://localhost:8081/health || exit 1
      # start_period: 5s
      # interval: 15s
      # timeout: 5s
    environment:
      SYNAPSE_WORKER: synapse.app.generic_worker
    # ports:
    #   - 8081:8081
    depends_on:
      - synapse

  synapse-federation-sender-1:
    <<: *synapse-template
    entrypoint: ["/start.py", "run", "--config-path=/data/homeserver.yaml", "--config-path=/data/workers/synapse-federation-sender-1.yaml"]
    healthcheck:
      disable: true
    environment:
      SYNAPSE_WORKER: synapse.app.federation_sender
    # ports:
    #   - 8081:8081
    depends_on:
      - synapse

  mas:
    image: ghcr.io/element-hq/matrix-authentication-service:$MAS_VERSION
    user: $USER_ID:$GROUP_ID
    restart: always
    # ports:
    #   - 8083:8080
    volumes:
      - ${VOLUME_PATH}/data/mas:/data:z
      - ${VOLUME_PATH}/data/caddy/data/caddy/pki/authorities/local/root.crt:/etc/ssl/certs/ca-certificates.crt:ro,z
      - db_sock_mas:/var/run/postgresql
    networks:
      - backend
    # FIXME: do we also need to sync the db?
    command: "server --config=/data/config.yaml"
    env_file: .env
    depends_on:
      db-mas:
        condition: service_healthy

  # as a basic local MTA
  mailhog:
    image: docker.io/mailhog/mailhog:latest
    restart: always
    ports:
      - 8025:8025
      # - 1025:1025
    networks:
      - backend

  element-web:
    image: docker.io/vectorim/element-web:$ELEMENT_WEB_VERSION
    restart: always
    # ports:
    #   - 8080:80
    healthcheck:
      test: wget -q -O /dev/null http://localhost:80/version || exit 1
      # start_period: 5s
      interval: 60s
      # timeout: 5s
    networks:
      - backend
    volumes:
      - ${VOLUME_PATH}/data/element-web/config.json:/app/config.json:ro,z

  element-call:
    image: ghcr.io/element-hq/element-call:$ELEMENT_CALL_VERSION
    restart: always
    # ports:
    #   - 8082:80
    healthcheck:
      test: curl -fSs http://localhost:8080/config.json || exit 1
      interval: 60s
    networks:
      - backend
    volumes:
      - ${VOLUME_PATH}/data/element-call/config.json:/app/config.json:ro,z

  livekit:
    image: docker.io/livekit/livekit-server:$LIVEKIT_VERSION
    restart: always
    volumes:
      - ${VOLUME_PATH}/data/livekit/config.yaml:/etc/livekit.yaml:ro,z
    command: --config /etc/livekit.yaml --node-ip ${LIVEKIT_NODE_IP}
    ports:
      # - 7880:7880 # HTTP listener
      - 7881:7881 # TCP WebRTC transport, advertised via SDP

      # TODO: expose livekit-turn on TCP & UDP 443 via nginx
      # At least this would allow UDP turn on port 443 for better perf.

      # You can't expose a massive range here as it literally sets up 10,000 userland listeners, which takes forever
      # and will clash with any existing high-numbered ports.
      # So for now, tunnel everything via TCP 7881. FIXME!
      #- 50000-60000:50000-60000/tcp # TCP media
      #- 50000-60000:50000-60000/udp # UDP media
    networks:
      - backend
    depends_on:
      redis:
        condition: service_started

  livekit-jwt:
    build: ./livekit-jwt
    restart: always
    volumes:
      - ${VOLUME_PATH}/data/caddy/data/caddy/pki/authorities/local/root.crt:/etc/ssl/certs/ca-certificates.crt:ro,z
      - ${VOLUME_PATH}/livekit-jwt/entrypoint.sh:/entrypoint.sh
    entrypoint: /entrypoint.sh
    env_file: .env
    networks:
      - backend
    secrets:
      - livekit_api_key
      - livekit_secret_key
    depends_on:
      livekit:
        condition: service_started

volumes:
  caddy_config:
  db_sock_synapse:
  db_sock_mas:
